{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5章　リカレントニューラルネットワーク (RNN)\n",
    "これまで見てきたニューラルネットワークは， **フィードフォワード**（入力信号が一方向だけに伝達されていく）であった．  \n",
    "一方でフィードフォワードは，時系列データをうまく扱えない（時系列データのパターンを十分に学習することができない）問題がある．  \n",
    "そのため，本章では **リカレントニューラルネットワーク** (Recurrent Neural Network, RNN) を解説・実装していく．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1　確率と言語モデル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずは前章までで扱ったword2vecを復習する．また，言語を確率として扱う「言語モデル」についても解説する．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1　word2vecを確率の視点から眺める"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずはCBOWモデルを復習する．ここでは， $w_1, \\cdots, w_T$という単語の列で表されるコーパスを考える．\n",
    "$t$番目の単語をターゲットとして，その前後の単語（$t-1$番目と$t+1$番目）をコンテキストとして扱う．\n",
    "図5-1を参照．  \n",
    "この確率は数式で表すと以下の通り．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "P(w_t | w_{t-1}, w_{t+1}) \\tag{5.1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CBOWモデルは(5.1)式の事後確率をモデル化する．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これまでは左右対称のウィンドウを考えてきたが，コンテキストとして左のウィンドウのみに限定することもできる．図5-2を参照．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "P(w_t | w_{t-2}, w_{t-1}) \\tag{5.2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここでは，ハイパーパラメータであるウィンドウサイズについて「左を2単語，右を0単語」と設定した．\n",
    "この理由は，後ほど説明する「言語モデル」を見越してのことだ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "損失関数は以下の通り．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "L = -\\log P(w_t | w_{t-2}, w_{t-1}) \\tag{5.3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CBOWモデルの学習で行うことは，式(5.3)で表される損失関数（のコーパス全体での総和）を最小とする重みパラメータを見つけることだ．  \n",
    "学習の結果，副産物として，単語の意味がエンコードされた「単語の分散表現」が得られる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "では，CBOWモデルの本来の目的であった「コンテキストからターゲットを推測すること」は何かに利用できるだろうか？\n",
    "そこに「言語モデル」が関係する．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2　言語モデル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**言語モデル** (Language Model) は，単語の並びに対して確率を与える．  \n",
    "このモデルは，さまざまなアプリケーションで応用可能だ．機械翻訳や音声認識などが，代表例である．  \n",
    "また，言語モデルは新しい文章を生成する用途にも利用できる（7章を参照）．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下では，$w_1, \\cdots, w_m$の$m$個の単語からなる文章を考える．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同時確率$P(w_1, \\cdots, w_m)$は，事後確率を用いて以下のように分解できる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "P(w_1, \\cdots, w_m)\n",
    "= \\prod_{t=1}^m P(w_t | w_1, \\cdots, w_{t-1}) \\tag{5.4}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "事後確率のイメージは図5-3を参照．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(w_t | w_1, \\cdots, w_{t-1})$を表すモデルは， **条件付き言語モデル** (Conditional Language Model) と呼ばれる．\n",
    "条件付き言語モデルを指して，「言語モデル」と呼ぶ場合も多い．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.3　CBOWモデルを言語モデルに？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vecのCBOWモデルを，（無理やり）言語モデルに適用するには，以下のような近似を行う．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "P(w_1, \\cdots, w_m)\n",
    "= \\prod_{t=1}^m P(w_t | w_1, \\cdots, w_{t-1})\n",
    "\\sim \\prod_{t=1}^m P(w_t | w_{t-2}, w_{t-1}) \\tag{5.8}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "直前の2つの単語だけに依存して次の単語が決まるモデルであるため，これは「2階マルコフ連鎖」である．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "式(5.8)では，コンテキストとして2つの単語を用いたが，コンテキストのサイズは任意の長さに固定できる．\n",
    "コンテキストが短ければ解けないであろう場面は，図5-4を参照．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "では，CBOWモデルのコンテキストのサイズを20や30のように大きくすれば問題は解決するのだろうか？\n",
    "確かに，CBOWモデルのコンテキストのサイズはいくらでも大きくすることはできる．\n",
    "しかし，CBOWモデルではコンテキスト内の単語の並びが無視されるという問題がある．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "具体例として，図5-5を参照．  \n",
    "コンテキストの単語の並びも考慮するために中間層において連結を行う手もあるが，パラメータはコンテキストのサイズに比例して増加してしまう．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここでRNNの出番となる．RNNは，コンテキストがどれだけ長くても，そのコンテキストの情報を記憶するメカニズムを持つ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2　RNNとは"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNにあるRecurrentとは「何度も繰り返し起こること」を意味する．\n",
    "そのため，RNNは日本語では「循環するニューラルネットワーク」ということになる．\n",
    "まず初めに，「循環する」という言葉について考えてみよう．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1　循環するニューラルネットワーク"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "循環するためには「閉じた経路」が必要となる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNの特徴は，ループする経路を持つことだ．データが循環することにより，過去の情報を記憶しながら，最新のデータへと更新される．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNで用いられるレイヤを「RNNレイヤ」という名前で呼ぶことにする．\n",
    "図5-6を参照．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNレイヤはループする経路を持つ．このループする経路によって，データがレイヤ内を循環することができる．\n",
    "なお図5-6では，時刻を$t$として，$x_t$を入力としている．\n",
    "$x_t$は何らかのベクトルを想定する．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "図5-6では「分岐」が起きている．\n",
    "「分岐」とは，同じものがコピーされて分岐することを意味する．\n",
    "そして，その分岐した出力のひとつが自分自身への入力となる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また，以下では紙面の都合上，図5-7のようにレイヤを90度回転させて描画することとする．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2　ループの展開"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "図5-8に示すように，RNNレイヤのループを展開することで，右方向に伸びる長いニューラルネットワークへと変身させられる．\n",
    "これは，今まで見てきたフィードフォワード型ニューラルネットワークと同じ構造ではあるが，複数のRNNレイヤがすべて「同じレイヤ」であることが，今までとは異なる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "図5-8にある通り，各時刻のRNNレイヤは，そのレイヤへの入力とひとつ前のレイヤからの出力を受け取る．\n",
    "数式で書くと以下の通り．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "h_t = \\tanh(h_{t-1} W_h + x_t W_x + b) \\tag{5.9}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNでは重みが$W_x, W_h$の2つある．\n",
    "また，バイアスとして$b$が設定されている．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "式(5.9)より，RNNは$h$という「状態」を持っており，式(5.9)の形で更新されると解釈できる．\n",
    "これが，RNNレイヤは「状態を持つレイヤ」や「メモリ（記憶力）を持つレイヤ」であると言われる所以だ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNの$h$は「状態」を記憶し，時間が1ステップ（1単位）進むに従い，式(5.9)の形で更新される．\n",
    "本書でも他の文献と同様に，RNNの出力$h$を **隠れ状態** (hidden state) や **隠れ状態ベクトル** (hidden state vector) と呼ぶことにする．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多くの文献では，展開後のRNNレイヤは図5-9の左図のように描かれている．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.3　Backpropagation THrough Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "展開後のRNNは横方向に伸びたニューラルネットワークとみなすことができた．\n",
    "そのためRNNの学習も，通常のニューラルネットワークと同じ手順で実行できる．\n",
    "図5-10を参照．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
